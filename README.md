# Responsible AI Dataset

**Author** - Anastasiia Saenko

*Large language models (LLMs) increasingly shape public discourse, yet their tendency to amplify misinformation and biases remains a critical challenge. This project introduces a comprehensive, metadata-enriched dataset designed to trace the origins of AI-generated content, identify biases, and assess misinformation propagation. By integrating diverse real-world sources—including verified news, social media, and misinformation repositories—this dataset facilitates the study of AI behavior across public health, political discourse, conflict narratives, and social issues. Our methodology combines data collection, enrichment, and analytical pipelines, incorporating misinformation labels, political bias indicators, sentiment analysis, and demographic markers. This structured approach enhances AI transparency and explainability, revealing how models rely on and transform specific sources. Additionally, we demonstrate how this dataset can inform bias mitigation strategies, prompt engineering frameworks, and Responsible AI benchmarking.* 

This work has practical and ethical significance in: 
1) Detecting and mitigating biases in AI-generated content,
2) Enhancing transparency by tracing how AI models utilize and amplify information from specific sources
3)  Advancing Responsible AI practices with real-world applications in public health, political discourse, and media integrity.

The primary contribution of this project is a robust dataset, complemented by analytical pipelines and prompt engineering frameworks that deepen our understanding of AI behavior and societal alignment. This research holds practical implications for media integrity, political transparency, and AI governance, offering tools for researchers and policymakers to assess and regulate the societal impact of AI-generated content.
